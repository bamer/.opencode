--- /dev/null
+++ b/src/watcher/launcher.py
@@ -0,0 +1,128 @@
+#!/usr/bin/env python3
+\"\"\"\n+Local watcher launcher for OpenAI-compatible endpoints.\n+\n+Runs watcher_loop.py prompt in a loop, sends it to a local OpenAI-compatible\n+server, logs the response, and stops when STOP_FILE is present or watcher\n+status indicates completion.\n+\"\"\"\n+
+from __future__ import annotations\n+
+import json\n+import os\n+import subprocess\n+import sys\n+import time\n+from pathlib import Path\n+from typing import Any, Dict\n+from urllib.error import HTTPError, URLError\n+from urllib.request import Request, urlopen\n+
+ROOT_DIR = Path(__file__).resolve().parents[1]\n+if str(ROOT_DIR) not in sys.path:\n+    sys.path.insert(0, str(ROOT_DIR))\n+
+from elf_paths import get_base_path\n+
+COORDINATION_DIR = Path(os.environ.get(\"ELF_BASE_PATH\", str(get_base_path()))) / \".coordination\"\n+WATCHER_LOG = COORDINATION_DIR / \"watcher-log.md\"\n+STOP_FILE = COORDINATION_DIR / \"watcher-stop\"\n+
+DEFAULT_BASE_URL = \"http://localhost:12134/v1\"\n+DEFAULT_MODEL = \"nemotron-v3-coder\"\n+DEFAULT_INTERVAL = 30\n+
+def resolve_base_url() -> str:\n+    base_url = os.environ.get(\"OPENCODE_WATCHER_BASE_URL\")\n+    if not base_url:\n+        base_url = os.environ.get(\"OPENAI_BASE_URL\")\n+    return (base_url or DEFAULT_BASE_URL).rstrip(\"/\")\n+
+def resolve_model() -> str:\n+    return os.environ.get(\"OPENCODE_WATCHER_MODEL\") or DEFAULT_MODEL\n+
+def resolve_interval() -> int:\n+    value = os.environ.get(\"OPENCODE_WATCHER_INTERVAL\")\n+    if not value:\n+        return DEFAULT_INTERVAL\n+    try:\n+        return int(value)\n+    except ValueError:\n+        return DEFAULT_INTERVAL\n+
+def fetch_prompt() -> str:\n+    script_path = Path(__file__).with_name(\"watcher_loop.py\")\n+    result = subprocess.run(\n+        [sys.executable, str(script_path), \"prompt\"],\n+        check=True,\n+        capture_output=True,\n+        text=True,\n+    )\n+    return result.stdout.strip()\n+
+def call_openai(base_url: str, model: str, prompt: str) -> str:\n+    url = f\"{base_url}/chat/completions\"\n+    api_key = os.environ.get(\"OPENCODE_WATCHER_API_KEY\") or os.environ.get(\"OPENAI_API_KEY\")\n+    headers = {\"Content-Type\": \"application/json\"}\n+    if api_key:\n+        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n+\n+    payload: Dict[str, Any] = {\n+        \"model\": model,\n+        \"messages\": [\n+            {\"role\": \"system\", \"content\": \"You are a monitoring agent for ELF watcher.\"},\n+            {\"role\": \"user\", \"content\": prompt},\n+        ],\n+        \"temperature\": 0.2,\n+    }\n+\n+    request = Request(url, data=json.dumps(payload).encode(\"utf-8\"), headers=headers)\n+    try:\n+        with urlopen(request, timeout=300) as response:\n+            data = json.loads(response.read().decode(\"utf-8\"))\n+    except (HTTPError, URLError) as exc:\n+        raise RuntimeError(f\"Watcher request failed: {exc}\") from exc\n+\n+    choices = data.get(\"choices\") or []\n+    if not choices:\n+        raise RuntimeError(\"Watcher response missing choices\")\n+    return choices[0].get(\"message\", {}).get(\"content\", \"\").strip()\n+
+def append_log(text: str) -> None:\n+    COORDINATION_DIR.mkdir(parents=True, exist_ok=True)\n+    WATCHER_LOG.write_text(WATCHER_LOG.read_text() + text + \"\\n\" if WATCHER_LOG.exists() else text + \"\\n\")\n+
+def should_stop(response: str) -> bool:\n+    if STOP_FILE.exists():\n+        return True\n+    for line in response.splitlines():\n+        if line.strip().upper().startswith(\"STATUS:\"):\n+            status = line.split(\":\", 1)[-1].strip().lower()\n+            if status in {\"complete\", \"stopped\"}:\n+                return True\n+    return False\n+
+def main() -> int:\n+    base_url = resolve_base_url()\n+    model = resolve_model()\n+    interval = resolve_interval()\n+    once = \"--once\" in sys.argv\n+\n+    while True:\n+        if STOP_FILE.exists():\n+            return 0\n+        prompt = fetch_prompt()\n+        response = call_openai(base_url, model, prompt)\n+        append_log(response)\n+        if should_stop(response) or once:\n+            return 0\n+        time.sleep(interval)\n+
+if __name__ == \"__main__\":\n+    raise SystemExit(main())