--- a/src/watcher/launcher.py
+++ b/src/watcher/launcher.py
@@ -1,4 +1,4 @@
 #!/usr/bin/env python3
 """
-Local watcher launcher for OpenAI-compatible endpoints.
+OpenCode watcher launcher using big-pickle model.
 
-Used by the watcher system to monitor coordination state.
+Uses the opencode CLI with big-pickle model for both Tier 1 (watcher) and Tier 2 (handler).
 """
@@ -7,83 +7,97 @@ import json
 import subprocess
 import sys
 import time
 from pathlib import Path
-from typing import Dict
-from urllib.error import HTTPError, URLError
-from urllib.request import Request, urlopen
+from typing import Any, Dict, Tuple
 
 ROOT_DIR = Path(__file__).resolve().parents[1]
 if str(ROOT_DIR) not in sys.path:
     sys.path.insert(0, str(ROOT_DIR))
 
 from elf_paths import get_base_path
 
 COORDINATION_DIR = Path(os.environ.get("ELF_BASE_PATH", str(get_base_path()))) / ".coordination"
 WATCHER_LOG = COORDINATION_DIR / "watcher-log.md"
 STOP_FILE = COORDINATION_DIR / "watcher-stop"
 
-DEFAULT_BASE_URL = "http://localhost:12134/v1"
-DEFAULT_MODEL = "nemotron-v3-coder"
+DEFAULT_MODEL = "opencode/big-pickle"
 DEFAULT_INTERVAL = 30
 
 
-def resolve_base_url() -> str:
-    base_url = os.environ.get("OPENCODE_WATCHER_BASE_URL")
-    if not base_url:
-        base_url = os.environ.get("OPENAI_BASE_URL")
-    return (base_url or DEFAULT_BASE_URL).rstrip("/")
-
 def resolve_model() -> str:
+    """Resolve the OpenCode model to use."""
     return os.environ.get("OPENCODE_WATCHER_MODEL") or DEFAULT_MODEL
 
 def resolve_interval() -> int:
+    """Resolve the watcher interval."""
     value = os.environ.get("OPENCODE_WATCHER_INTERVAL")
     if not value:
         return DEFAULT_INTERVAL
     try:
         return int(value)
     except ValueError:
         return DEFAULT_INTERVAL
 
 
 def fetch_prompt() -> str:
+    """Fetch watcher prompt from watcher_loop.py."""
     script_path = Path(__file__).with_name("watcher_loop.py")
     result = subprocess.run(
-        [sys.executable, str(script_path), "prompt"],
+        [sys.executable, str(script_path), prompt_type],
         check=True,
         capture_output=True,
         text=True,
     )
     return result.stdout.strip()
 
-def call_openai(base_url: str, model: str, prompt: str) -> str:
-    url = f"{base_url}/chat/completions"
-    api_key = os.environ.get("OPENCODE_WATCHER_API_KEY") or os.environ.get("OPENAI_API_KEY")
-    headers = {"Content-Type": "application/json"}
-    if api_key:
-        headers["Authorization"] = f"Bearer {api_key}"
+def call_opencode(model: str, prompt: str) -> Tuple[str, bool]:
+    """Call opencode CLI with the given model and prompt."""
+    try:
+        result = subprocess.run(
+            ["opencode", "--model", model, "--prompt", prompt],
+            capture_output=True,
+            timeout=300,
+            text=True,
+        )
+        
+        if result.returncode == 0:
+            return result.stdout, True
+        else:
+            return f"Error: opencode returned error code {result.returncode}\n{result.stderr}", False
+    except subprocess.TimeoutExpired:
+        return "Error: opencode request timed out (>300s)", False
+    except FileNotFoundError:
+        return "Error: opencode CLI not found. Install with: npm install -g opencode", False
+    except Exception as e:
+        return f"Error calling opencode: {e}", False
 
-    payload: Dict[str, Any] = {
-        "model": model,
-        "messages": [
-            {"role": "system", "content": "You are a monitoring agent for ELF watcher."},
-            {"role": "user", "content": prompt},
-        ],
-        "temperature": 0.2,
-    }
-
-    request = Request(url, data=json.dumps(payload).encode("utf-8"), headers=headers)
-    try:
-        with urlopen(request, timeout=300) as response:
-            data = json.loads(response.read().decode("utf-8"))
-    except (HTTPError, URLError) as exc:
-        raise RuntimeError(f"Watcher request failed: {exc}") from exc
-
-    choices = data.get("choices") or []
-    if not choices:
-        raise RuntimeError("Watcher response missing choices")
-    return choices[0].get("message", {}).get("content", "").strip()
 
 def append_log(text: str) -> None:
+    """Append text to watcher log."""
     COORDINATION_DIR.mkdir(parents=True, exist_ok=True)
-    WATCHER_LOG.write_text(WATCHER_LOG.read_text() + text + "\n" if WATCHER_LOG.exists() else text + "\n")
+    existing = WATCHER_LOG.read_text() if WATCHER_LOG.exists() else ""
+    WATCHER_LOG.write_text(existing + text + "\n")
 
 def should_stop(response: str) -> bool:
+    """Check if watcher should stop."""
     if STOP_FILE.exists():
         return True
     for line in response.splitlines():
         if line.strip().upper().startswith("STATUS:"):
             status = line.split(":", 1)[-1].strip().lower()
             if status in {"complete", "stopped"}:
                 return True
     return False
 
-def main() -> int:
-    base_url = resolve_base_url()
+def run_single_pass(model: str) -> int:
+    """Run one watcher monitoring pass."""
+    print(f"[WATCHER] Fetching prompt...", file=sys.stderr)
+    
+    # Get watcher prompt
+    prompt = fetch_prompt()
+    print(f"[WATCHER] Sending to {model}...", file=sys.stderr)
+    
+    # Call OpenCode with watcher prompt
+    response, success = call_opencode(model, prompt)
+    
+    if not success:
+        print(f"Error: {response}", file=sys.stderr)
+        append_log(f"[TIER 1 ERROR] {response}")
+        return 2
+    
+    # Output and log response
+    print(response)
+    append_log(f"[TIER 1 WATCHER] {response[:200]}")
+    
+    # Check if escalation needed
+    if should_stop(response):
+        return 0
+    
+    # Check if handler escalation needed
+    needs_handler = any(x in response for x in ["STATUS: error", "STATUS: stale", "STATUS: complete"])
+    
+    if needs_handler:
+        print(f"\n[HANDLER] Escalating to handler (Tier 2)...", file=sys.stderr)
+        
+        # Get handler prompt
+        handler_prompt = fetch_prompt("handler-prompt")
+        handler_response, success = call_opencode(model, handler_prompt)
+        
+        if not success:
+            print(f"Error: {handler_response}", file=sys.stderr)
+            append_log(f"[TIER 2 ERROR] {handler_response}")
+            return 2
+        
+        # Output and log handler response
+        print(f"\n{handler_response}")
+        append_log(f"[TIER 2 HANDLER] {handler_response[:200]}")
+        
+        # Return 1 if ESCALATE action was taken
+        if "ESCALATE" in handler_response:
+            return 1
+    
+    return 0
+
+
+def main() -> int:
+    """Main entry point."""
     model = resolve_model()
-    interval = resolve_interval()
+    interval = resolve_interval()
     once = "--once" in sys.argv
 
+    print(f"Starting OpenCode Watcher with model: {model}", file=sys.stderr)
+    print(f"Interval: {interval}s", file=sys.stderr)
+
     while True:
         if STOP_FILE.exists():
+            print("Stop file detected, exiting.", file=sys.stderr)
             return 0
-        prompt = fetch_prompt()
-        response = call_openai(base_url, model, prompt)
-        append_log(response)
-        if should_stop(response) or once:
-            return 0
-        time.sleep(interval)
+        
+        exit_code = run_single_pass(model)
+        
+        if exit_code != 0 or once:
+            return exit_code
+        
+        print(f"Sleeping {interval}s...", file=sys.stderr)
+        time.sleep(interval)
 
 if __name__ == "__main__":
     raise SystemExit(main())
